<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/functions.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemA1.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemA2.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemA3.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemA4.R")
@
<<echo=FALSE>>=
source("../code/functions.R")
library(ggplot2)
library(tibble)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{1.}
We consider the exponential distribution with parameter $\lambda > 0$, and wish to generate samples from this. The probability density function and cumulative density function are
%
\begin{equation}
\label{eq:exp}
   f(x) =  \lambda e^{-\lambda x} \quad \text{and} \quad F(x) = 1 - e^{-\lambda x} \, ,
\end{equation}
%
respectively. Let $F(x) = u$. The inverse $F^{-1}$ of $F$ is given by
%
\begin{equation}
\label{eq:inv_exp}
    x = F^{-1}(u) = -\frac{1}{\lambda}\log (1-u) \, .
\end{equation}
Using the inverse and letting $u \sim \mathcal{U}[0,1]$ (and so also $1-u \sim \mathcal{U}[0,1]$), we generate a vector of $n$ samples and compare it with the exact exponential distribution in the code below.
%
    <<r_exp, eval=FALSE>>=
@
%
Next we test \texttt{r\_exp()}.
%
<<A1, fig.align='center', fig.width=6, fig.height=4, fig.cap="Theoretical density (red line) from a exponential distribution with parameter $\\lambda = 0.5$ and frequency histogram of simulations.">>==
@
%
In Figure \ref{fig:A1} we can see that the histogram of the samples follow the exponential distribution using the same $\lambda$. The code also outputs the maximum relative error of the mean and variance compared to the true values $\lambda^{-1}$ and $\lambda^{-2}$, respectively. As we can see, the errors are small.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{2.}
\paragraph{(a)}
We consider the density function $g(x)$ given in 2. The cumulative density function
$G(x) = \int_{-\infty}^{x} g(x) \dif x$ is
%
\begin{equation}
G(x) =
\begin{cases}
0, & x \leq 0 \\
c\alpha^{-1}x^\alpha, & 0\ < x < 1 \\
c(\alpha^{-1} + e^{-1}-e^{-x}), & 1\leq x
\end{cases} \, ,
\end{equation}
%
where $\alpha \in (0,1)$. This gives the inverse of G(x)
%
\begin{equation}
G^{-1}(u) = 
\begin{cases}
\left(\frac{u\alpha}{c}\right)^{\frac{1}{\alpha}}, & 0 < u \leq c\alpha^{-1} \\
\log \left[c(1-u)^{-1}\right], & c\alpha^{-1} < u \leq 1
\end{cases} \, .
\end{equation}
%
We find the expected value $\E(x) = \int_0^\infty x g(x) \dif x$:
%
\begin{equation}
    \E(x) = c\left[\frac{1}{\alpha+1}+2e^{-1}\right] \, . 
\label{eq:expected_A}
\end{equation}
%
The variance one can find from the equation $\Var(x) = \E(x^2)-\left[\E(x)\right]^2$. Then by calculating $\E(x^2)$ the same way as the expected value in \eqref{eq:expected_A}, we get
%
\begin{equation}
    \Var(x) = c\left[\frac{1}{\alpha+2}+5e^{-1}\right] -c^2\left[ \frac{1}{\alpha+1}+ 2e^{-1}\right].
\end{equation}
The normalizing constant $c$ is given by solving $\int_0^\infty g(x) \dif x= 1$ for $c$, which yields 
$ c = \frac{e \alpha}{e + \alpha}$.

\paragraph{(b)}
We wish to generate samples from $g$ and compare it to the theoretical distribution. The function \texttt{rg()} below generates a vector of n independent samples from $g$ with $\alpha \in (0, 1)$.
%
<<g, eval=FALSE>>=
@
\vspace{-1em}
<<rg, eval=FALSE>>=
@
%
Next we test \texttt{rg()}.
%
<<A2, fig.align='center', fig.width=6, fig.height=4, fig.cap="Theoretical density (red line) from the $g(x)$ distribution with $\\alpha = 0.5$ and frequency histogram of simulations.">>==
@
%
In Figure \ref{fig:A2} we see that the histogram height follows the theoretical distribution of g(x) using $\alpha = 0.5$. Also the code test of \texttt{rg()} outputs the maximum relative error and variance using different values of $\alpha$. The maximum error is less than 3\%, which indicates that the samples are indeed from the distribution $g(x)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{3.}
%
We now wish to generate $n$ samples of two standard normal distribution using the Box-Muller algorithm, which consists of generating two uniformly distributed variables in polar coordinates and then transforming them into Cartesian coordinates. We will check if these are indeed then normal distributed.
%
<<r_boxmuller, eval=FALSE>>=
@
%
Next we test \texttt{r\_boxmuller()}.
%
<<A3, fig.align='center', fig.width=7, fig.height=4, fig.cap="Theoretical density (red lines) from the standard normal distribution with an frequency histogram of a simulation using the Box-Muller algorithm.">>==
@
%
In Figure \ref{fig:A3} we see that the frequency plot of the two sample follow the theoretical standard normal distribution $\mathcal{N}[0,1]$. The empirical mean and variance are \Sexpr{format(round(empirical_mean, 4), nsmall=4)} and \Sexpr{format(round(empirical_var, 4), nsmall=4)}, respectively. We conclude that the Box-Muller algorithm successfully generates samples from a standard normal distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{4.}
%
We wish to generate $n$ samples from a $d$-variate normal distribution with a given mean $\mu$ and covariance $\Sigma$. We use the Box-Muller algorithm to generate $n$ samples of $d$ standard normal distribution ($\vect{x} \sim \mathcal{N}_d[0,1]$). Then we use that, if $A A^T = \Sigma$, $\vect{y} = \vect{\mu} + A\vect{x} \sim \mathcal{N}_d[\vect{\mu}, \Sigma]$.

We set $\mu \sim \mathcal{U}_d[0,10]$ and $A$ to be a $(d \times d)$ matrix of uniformly distributed elements ($a_{ij} = \mathcal{U}[0,1]$). Then we choose as covariance matrix $\Sigma = AA^T$, which ensures it is symmetric and positive definite. 
<<r_multinorm, eval=FALSE>>=
@
%
Next we test \texttt{r\_multinorm()}.
%
<<A4>>==
@
%
We see from the outputs of the maximum relative error of the mean and covariances, that the algorithm works as expected; the errors are very small.
% maybe have divariate plot of this?
